{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f56a9b3",
      "metadata": {},
      "source": [
        "# Upstream Data Upload Guide\n",
        "\n",
        "## Overview\n",
        "\n",
        "This guide demonstrates how to authenticate with the Upstream API and upload sensor data using CSV files for environmental monitoring campaigns.\n",
        "\n",
        "## What You Can Do\n",
        "\n",
        "The Upstream API allows you to:\n",
        "- Authenticate and obtain access tokens\n",
        "- Upload sensor definitions and measurement data\n",
        "- Manage environmental monitoring campaigns\n",
        "- Query and retrieve measurement data\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Valid Upstream account credentials\n",
        "- Python 3.7+ with `requests` library installed\n",
        "- CSV files with sensor and measurement data formatted correctly\n",
        "\n",
        "## Installation\n",
        "\n",
        "```bash\n",
        "pip install requests\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dee1efa",
      "metadata": {},
      "source": [
        "## Quick Start\n",
        "\n",
        "1. **Authenticate** with the API to get your access token\n",
        "2. **Prepare your CSV files** following the required format\n",
        "3. **Upload your data** using the provided functions\n",
        "4. **Monitor the results** and verify successful upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "3de5ed4d-505a-4a59-b15a-7de41e8246d1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tapipy in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (1.8.5)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.8.0 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from tapipy) (4.17.3)\n",
            "Requirement already satisfied: PyJWT>=1.7.1 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from tapipy) (2.10.1)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from tapipy) (6.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.6.0 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from tapipy) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2020.11.8 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from tapipy) (2025.4.26)\n",
            "Requirement already satisfied: cryptography>=3.3.2 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from tapipy) (45.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.20.0 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from tapipy) (2.32.3)\n",
            "Requirement already satisfied: openapi_core==0.16.0 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from tapipy) (0.16.0)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from tapipy) (58.1.0)\n",
            "Requirement already satisfied: six<2.0,>=1.10 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from tapipy) (1.17.0)\n",
            "Requirement already satisfied: python_dateutil<3.0.0,>=2.5.3 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from tapipy) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.26.5 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from tapipy) (1.26.20)\n",
            "Requirement already satisfied: atomicwrites<2.0.0,>=1.4.0 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from tapipy) (1.4.1)\n",
            "Requirement already satisfied: openapi_spec_validator<0.6.0,>=0.5.0 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from tapipy) (0.5.4)\n",
            "Requirement already satisfied: more-itertools in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from openapi_core==0.16.0->tapipy) (10.7.0)\n",
            "Requirement already satisfied: openapi-schema-validator<0.4.0,>=0.3.0 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from openapi_core==0.16.0->tapipy) (0.3.4)\n",
            "Requirement already satisfied: parse in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from openapi_core==0.16.0->tapipy) (1.20.2)\n",
            "Requirement already satisfied: pathable<0.5.0,>=0.4.0 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from openapi_core==0.16.0->tapipy) (0.4.4)\n",
            "Requirement already satisfied: jsonschema-spec<0.2.0,>=0.1.1 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from openapi_core==0.16.0->tapipy) (0.1.6)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.3.0 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from openapi_core==0.16.0->tapipy) (4.14.0)\n",
            "Requirement already satisfied: werkzeug in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from openapi_core==0.16.0->tapipy) (3.1.3)\n",
            "Requirement already satisfied: isodate in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from openapi_core==0.16.0->tapipy) (0.7.2)\n",
            "Requirement already satisfied: cffi>=1.14 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from cryptography>=3.3.2->tapipy) (1.17.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from jsonschema<5.0.0,>=4.8.0->tapipy) (0.20.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from jsonschema<5.0.0,>=4.8.0->tapipy) (25.3.0)\n",
            "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from openapi_spec_validator<0.6.0,>=0.5.0->tapipy) (1.11.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.20.0->tapipy) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.20.0->tapipy) (3.4.2)\n",
            "Requirement already satisfied: pycparser in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from cffi>=1.14->cryptography>=3.3.2->tapipy) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/wmobley/Documents/GitHub/upstream-docker/.venv/lib/python3.9/site-packages (from werkzeug->openapi_core==0.16.0->tapipy) (3.0.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! pip install tapipy\n",
        "import requests\n",
        "import json\n",
        "import getpass\n",
        "import os\n",
        "from tapipy.tapis import Tapis\n",
        "from typing import Dict, Any, Optional, List\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65443e09",
      "metadata": {},
      "source": [
        "## 1. Authentication\n",
        "\n",
        "First, we need to authenticate with the Upstream API to obtain an access token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "7b250831-bec9-4425-b165-127e49d76ffc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Authentication successful!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "credentials = {\n",
        "        \"username\": input(\"Username: \"),\n",
        "        \"password\": getpass.getpass(\"Password: \")\n",
        "    }\n",
        "def authenticate_upstream(base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\") -> str:\n",
        "    \"\"\"\n",
        "    Authenticate with Upstream API and return access token.\n",
        "    Args:\n",
        "        base_url: Base URL for the Upstream API (dev or prod)\n",
        "    Returns:\n",
        "        Access token string\n",
        "    Raises:\n",
        "        Exception: If authentication fails\n",
        "    \"\"\"\n",
        "    auth_url = f\"{base_url}/api/v1/token\"\n",
        "    try:\n",
        "        response = requests.post(auth_url, data=credentials)\n",
        "        response.raise_for_status()    \n",
        "        token = response.json().get(\"access_token\")\n",
        "        if not token:\n",
        "            raise Exception(\"No access token in response\")\n",
        "        print(\"✅ Authentication successful!\")\n",
        "        return token\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        raise Exception(f\"Authentication failed: {e}\")\n",
        "\n",
        "# Get authentication token\n",
        "token = authenticate_upstream()\n",
        "# Create python Tapis client for user\n",
        "t = Tapis(base_url= \"https://portals.tapis.io\",\n",
        "          username=credentials['username'],\n",
        "          password=credentials['password'])\n",
        "\n",
        "# Call to Tokens API to get access token\n",
        "t.get_tokens()\n",
        "tapis_token = t.access_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9c608a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_authenticated_request(\n",
        "    method: str,\n",
        "    url: str,\n",
        "    token: str,\n",
        "    json: Optional[Dict] = None,\n",
        "    files: Optional[Dict] = None,\n",
        "    params: Optional[Dict] = None\n",
        ") -> requests.Response:\n",
        "    \"\"\"\n",
        "    Make an authenticated HTTP request to the Upstream API.\n",
        "    \n",
        "    Args:\n",
        "        method: HTTP method (GET, POST, PUT, DELETE, etc.)\n",
        "        url: Full URL for the request\n",
        "        token: Authentication token\n",
        "        json: JSON data for the request body\n",
        "        files: Files for multipart upload\n",
        "        params: URL parameters\n",
        "        \n",
        "    Returns:\n",
        "        Response object from the request\n",
        "        \n",
        "    Raises:\n",
        "        requests.exceptions.HTTPError: If the request fails\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "    }\n",
        "    \n",
        "    # Don't set Content-Type for file uploads (requests will set it automatically)\n",
        "    if files is None:\n",
        "        headers[\"Content-Type\"] = \"application/json\"\n",
        "    try:\n",
        "        response = requests.request(\n",
        "            method=method.upper(),\n",
        "            url=url,\n",
        "            headers=headers,\n",
        "            json=json,\n",
        "            files=files,\n",
        "            params=params,\n",
        "            timeout=300  # 5 minute timeout for large file uploads\n",
        "        )\n",
        "        \n",
        "        # Raise an exception for bad status codes\n",
        "        response.raise_for_status()\n",
        "        return response\n",
        "        \n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"❌ HTTP Error: {e}\")\n",
        "        print(f\"Response content: {response.text}\")\n",
        "        raise\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Request Error: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ede83720",
      "metadata": {},
      "source": [
        "## 2. Helper Functions for API Requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a20206c-dc87-4f0d-b9cf-87923998a9f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_campaign(\n",
        "    campaign_data:str,    \n",
        "    token: str,\n",
        "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Create a new campaign.\n",
        "    \n",
        "    Args:\n",
        "        name: Campaign name\n",
        "        description: Campaign description\n",
        "        allocation: TACC allocation identifier (required)\n",
        "        token: Authentication token\n",
        "        base_url: Base URL for the API\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary containing the created campaign data with ID\n",
        "    \"\"\"\n",
        "    url = f\"{base_url}/api/v1/campaigns\"    \n",
        "    response = make_authenticated_request(\n",
        "        method=\"POST\",\n",
        "        url=url,\n",
        "        token=token,\n",
        "        json=campaign_data\n",
        "    )\n",
        "    result = response.json()\n",
        "    print(f\"✅ Campaign created successfully!\")\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8f94d96",
      "metadata": {},
      "source": [
        "### Creating Campaigns\n",
        "\n",
        "Before uploading CSV data, you need to create a campaign to organize your data collection project. A campaign serves as the top-level container for all related monitoring activities.\n",
        "\n",
        "#### Campaign Requirements\n",
        "\n",
        "**Required Fields:**\n",
        "- `name`: Descriptive name for your data collection project\n",
        "- `description`: Detailed description of the campaign's purpose and scope\n",
        "\n",
        "#### Campaign Best Practices\n",
        "\n",
        "🎯 **Naming Conventions:**\n",
        "- Use descriptive, unique names that clearly identify the project\n",
        "- Include dates, locations, or project codes for easy identification\n",
        "- Examples: \"Austin Air Quality 2024\", \"Hurricane Harvey Recovery Monitoring\"\n",
        "\n",
        "📝 **Descriptions:**\n",
        "- Provide detailed context about the campaign's objectives\n",
        "- Include information about duration, scope, and expected outcomes\n",
        "- Mention any relevant research or operational goals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2e618b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Creating Campaign from Configuration ===\n",
            "📋 Campaign Configuration Summary:\n",
            "  Name: Beaumont Stream Gauge\n",
            "  Description: Beaumont Stream Gauge Campaign...\n",
            "✅ Campaign created successfully!\n",
            "Campaign ID: 12\n",
            "\n",
            "🎉 Campaign setup complete!\n",
            "Campaign ID: 12\n"
          ]
        }
      ],
      "source": [
        "def load_and_create_campaign(\n",
        "    config_path: str = \"campaigns/campaign.json\",\n",
        "    token: str = None,\n",
        "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Load campaign configuration from JSON and create the campaign.\n",
        "    \n",
        "    Args:\n",
        "        config_path: Path to the campaign configuration JSON file\n",
        "        token: Authentication token\n",
        "        base_url: Base URL for the API\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary containing the created campaign data with ID\n",
        "    \"\"\"\n",
        "    # Load configuration\n",
        "    with open(config_path) as campaign_data:\n",
        "        campaign_json = json.loads(campaign_data.read())\n",
        "\n",
        "    # Validate required fields\n",
        "    required_fields = [\"name\", \"description\"]\n",
        "    for field in required_fields:\n",
        "        if field not in campaign_json:\n",
        "            raise ValueError(f\"Missing required field '{field}' in campaign config\")    \n",
        "    # Display configuration summary\n",
        "    print(f\"📋 Campaign Configuration Summary:\")\n",
        "    print(f\"  Name: {campaign_json['name']}\")\n",
        "    print(f\"  Description: {campaign_json['description'][:100]}...\")\n",
        "    if \"metadata\" in campaign_json:\n",
        "        metadata = campaign_json[\"metadata\"]\n",
        "        print(f\"  Project Lead: {metadata.get('project_lead', 'N/A')}\")\n",
        "        print(f\"  Institution: {metadata.get('institution', 'N/A')}\")\n",
        "    \n",
        "    # Create the campaign\n",
        "    campaign = create_campaign(\n",
        "        campaign_data=campaign_json,\n",
        "        token=token,\n",
        "        base_url=base_url\n",
        "    )\n",
        "    return campaign\n",
        "\n",
        "try:\n",
        "    campaign = load_and_create_campaign(\n",
        "        config_path=\"campaigns/campaign.json\",\n",
        "        token=token\n",
        "    )    \n",
        "    campaign_id = campaign['id']\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ Configuration file error: {e}\")\n",
        "    print(\"💡 Please create a campaigns/campaign.json file with your campaign details\")\n",
        "except ValueError as e:\n",
        "    print(f\"❌ Configuration error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Campaign creation failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f364022",
      "metadata": {},
      "source": [
        "### Creating Stations\n",
        "\n",
        "Once you have a campaign, you need to create stations within it. Stations represent specific monitoring locations where sensors collect data.\n",
        "\n",
        "#### Station Requirements\n",
        "\n",
        "**Required Fields:**\n",
        "- `campaign_id`: ID of the parent campaign (must exist)\n",
        "- `name`: Unique name for the monitoring station\n",
        "- `description`: Details about the station location and purpose\n",
        "- `latitude`: Decimal degrees (e.g., 30.2672)\n",
        "- `longitude`: Decimal degrees (e.g., -97.7431)\n",
        "\n",
        "#### Station Best Practices\n",
        "\n",
        "📍 **Location Data:**\n",
        "- Ensure coordinates are in decimal degrees format\n",
        "- Use WGS84 coordinate system (standard GPS coordinates)\n",
        "- Verify coordinates are accurate for your monitoring location\n",
        "- Test coordinates in mapping software before creating stations\n",
        "\n",
        "🏷️ **Station Naming:**\n",
        "- Use descriptive names that indicate location or purpose\n",
        "- Include geographic references or landmarks\n",
        "- Examples: \"River Bridge Station\", \"Industrial District Monitor\"\n",
        "\n",
        "📝 **Station Descriptions:**\n",
        "- Describe the physical location and surroundings\n",
        "- Note any special characteristics or constraints\n",
        "- Include installation details or access information\n",
        "\n",
        "#### Alternative: Web Interface for Stations\n",
        "\n",
        "If you prefer using the web interface:\n",
        "\n",
        "1. **Navigate to Campaign:**\n",
        "   - Go to your created campaign in the web portal\n",
        "   - Access the campaign details page\n",
        "\n",
        "2. **Create Station:**\n",
        "   - Go to the \"Stations\" section within the campaign\n",
        "   - Click \"Add Station\"\n",
        "   - Provide station details and coordinates\n",
        "   - Save to get your Station ID\n",
        "\n",
        "3. **Note the Station ID:**\n",
        "   - Copy the Station ID for use in data uploads\n",
        "\n",
        "\n",
        "💡 **Pro Tip:** Save your campaign and station IDs in a configuration file or notebook cell for easy reuse across multiple data uploads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ee3af6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_station(\n",
        "    station_data: Dict[str, Any],\n",
        "    campaign_id: int,\n",
        "    token: str,\n",
        "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Create a new station within a campaign.\n",
        "    \n",
        "    Args:\n",
        "        station_data: Dictionary containing station information\n",
        "        campaign_id: ID of the parent campaign\n",
        "        token: Authentication token\n",
        "        base_url: Base URL for the API\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary containing the created station data with ID\n",
        "    \"\"\"\n",
        "    url = f\"{base_url}/api/v1/campaigns/{campaign_id}/stations\"    \n",
        "    response = make_authenticated_request(\n",
        "        method=\"POST\",\n",
        "        url=url,\n",
        "        token=token,\n",
        "        json=station_data\n",
        "    )\n",
        "    result = response.json()\n",
        "    print(f\"✅ Station created successfully!\")\n",
        "    print(f\"Station ID: {result.get('id')}\")\n",
        "    print(f\"Station Name: {station_data.get('name')}\")\n",
        "    print(f\"Project ID: {station_data.get('projectid')}\")\n",
        "    print(f\"Contact: {station_data.get('contact_name')}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "def load_station_config(config_path: str = \"stations/station.json\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Load station configuration from JSON file.\n",
        "    \n",
        "    Args:\n",
        "        config_path: Path to the station configuration JSON file    \n",
        "    Returns:\n",
        "        Dictionary containing station configuration data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(config_path, 'r', encoding='utf-8') as file:\n",
        "            config = json.load(file)\n",
        "            return config\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Station config file not found: {config_path}\")\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(f\"Invalid JSON in station config file: {e}\")\n",
        "\n",
        "def load_and_create_station(\n",
        "    campaign_id: int,\n",
        "    config_path: str = \"stations/station.json\",\n",
        "    token: str = None,\n",
        "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Load station configuration from JSON and create the station.\n",
        "    \n",
        "    Args:\n",
        "        campaign_id: ID of the parent campaign\n",
        "        config_path: Path to the station configuration JSON file\n",
        "        token: Authentication token\n",
        "        base_url: Base URL for the API\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary containing the created station data with ID\n",
        "    \"\"\"\n",
        "    # Load configuration\n",
        "    station_config = load_station_config(config_path)\n",
        "    # Validate required fields\n",
        "    required_fields = [\"name\", \"projectid\", \"description\", \"contact_name\", \"contact_email\", \"active\", \"start_date\"]\n",
        "    for field in required_fields:\n",
        "        if field not in station_config:\n",
        "            raise ValueError(f\"Missing required field '{field}' in station config\")\n",
        "    # Display configuration summary\n",
        "    print(f\"📋 Station Configuration Summary:\")\n",
        "    print(f\"  Name: {station_config['name']}\")\n",
        "    print(f\"  Project ID: {station_config['projectid']}\")\n",
        "    print(f\"  Description: {station_config['description'][:100]}...\")\n",
        "    print(f\"  Contact: {station_config['contact_name']}\")\n",
        "    print(f\"  Active: {station_config['active']}\")\n",
        "    print(f\"  Start Date: {station_config['start_date']}\")\n",
        "    # Create the station\n",
        "    station = create_station(\n",
        "        station_data=station_config,\n",
        "        campaign_id=campaign_id,\n",
        "        token=token,\n",
        "        base_url=base_url\n",
        "    )\n",
        "    return station\n",
        "\n",
        "def load_and_create_multiple_stations(\n",
        "    campaign_id: int,\n",
        "    config_path: str = \"stations/stations.json\",\n",
        "    token: str = None,\n",
        "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Load multiple station configurations from JSON and create all stations.\n",
        "    Args:\n",
        "        campaign_id: ID of the parent campaign\n",
        "        config_path: Path to the stations configuration JSON file\n",
        "        token: Authentication token\n",
        "        base_url: Base URL for the API\n",
        "        \n",
        "    Returns:\n",
        "        List of dictionaries containing the created station data\n",
        "    \"\"\"\n",
        "    # Load configuration\n",
        "    with open(config_path, 'r', encoding='utf-8') as file:\n",
        "        stations_config = json.load(file)\n",
        "    created_stations = []\n",
        "    # Handle both single station and multiple stations format\n",
        "    if \"stations\" in stations_config:\n",
        "        station_list = stations_config[\"stations\"]\n",
        "    else:\n",
        "        station_list = [stations_config]  # Single station format\n",
        "\n",
        "    print(f\"📋 Creating {len(station_list)} station(s)...\")\n",
        "    \n",
        "    for i, station_config in enumerate(station_list, 1):\n",
        "        print(f\"\\n--- Creating Station {i}/{len(station_list)} ---\")        \n",
        "        try:\n",
        "            station = create_station(\n",
        "                station_data=station_config,\n",
        "                campaign_id=campaign_id,\n",
        "                token=token,\n",
        "                base_url=base_url\n",
        "            )\n",
        "            created_stations.append(station)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to create station '{station_config.get('name', 'Unknown')}': {e}\")\n",
        "            continue\n",
        "    \n",
        "    return created_stations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0926cc6d",
      "metadata": {},
      "source": [
        "## 📡 Registering Environmental Monitoring Stations to CKAN\n",
        "The next section walks you through the process of automating the registration of environmental monitoring stations to a CKAN data portal. By using this code, you're streamlining the workflow of:\n",
        "\n",
        "- 🔐 Authenticating with CKAN using a JWT token\n",
        "\n",
        "- 🏷️ Creating datasets that represent sensor stations\n",
        "\n",
        "- 📎 Uploading metadata and resources such as sensor types, campaign info, and contact details\n",
        "\n",
        "- 📁 Organizing data for discoverability and reuse within research communities\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e40e595",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_ckan_dataset(\n",
        "    jwt_token: str,\n",
        "    dataset_name: str,\n",
        "    title: str,\n",
        "    description: str,\n",
        "    tags: list = None,\n",
        "    owner_org: str = None,\n",
        "    ckan_url: str = \"https://ckan.tacc.utexas.edu\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Create a dataset (package) in CKAN to represent a station.    \n",
        "    Args:\n",
        "        jwt_token: JWT authentication token\n",
        "        dataset_name: Unique dataset identifier (lowercase, no spaces)\n",
        "        title: Human-readable title\n",
        "        description: Dataset description\n",
        "        tags: List of tag names\n",
        "        owner_org: owner_org name/id\n",
        "        ckan_url: CKAN instance URL\n",
        "        \n",
        "    Returns:\n",
        "        CKAN API response\n",
        "    \"\"\"\n",
        "    \n",
        "    # Prepare dataset metadata\n",
        "    dataset_data = {\n",
        "        \"name\": dataset_name,\n",
        "        \"title\": title,\n",
        "        \"notes\": description,\n",
        "        \"tags\": [{\"name\": tag} for tag in (tags or [])],\n",
        "        \"private\": False,\n",
        "        \"type\": \"dataset\"\n",
        "    }\n",
        "    \n",
        "    dataset_data[\"owner_org\"] = owner_org\n",
        "    # CKAN API endpoint\n",
        "    api_url = f\"{ckan_url}/api/3/action/package_create\"\n",
        "    # Headers with JWT token\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {tapis_token.access_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            api_url,\n",
        "            headers=headers,\n",
        "            json=dataset_data\n",
        "        )        \n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        if result.get(\"success\"):\n",
        "            dataset_id = result[\"result\"][\"id\"]\n",
        "            dataset_url = f\"{ckan_url}/dataset/{dataset_name}\"   \n",
        "            return result[\"result\"]\n",
        "        else:\n",
        "            print(f\"❌ CKAN API returned error: {result}\")\n",
        "            raise Exception(f\"CKAN API error: {result}\")\n",
        "            \n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ HTTP request failed: {e}\")\n",
        "        if hasattr(e, 'response') and e.response is not None:\n",
        "            print(f\"   Response: {e.response.text}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Dataset creation failed: {e}\")\n",
        "        raise\n",
        "\n",
        "def register_station_to_ckan(\n",
        "    jwt_token:str,\n",
        "    station_name: str,\n",
        "    station_title: str,\n",
        "    station_description: str,\n",
        "    campaign_name: str = None,\n",
        "    sensor_types: list = None,\n",
        "    author:str=None,\n",
        "    author_email:str=None,\n",
        "    owner_org: str = None,\n",
        "    ckan_url: str = \"https://ckan.tacc.utexas.edu\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Complete workflow to register a station in CKAN.\n",
        "    Args:\n",
        "        username: Tapis username\n",
        "        password: Tapis password\n",
        "        station_name: Unique station identifier\n",
        "        station_title: Human-readable station title\n",
        "        station_description: Station description\n",
        "        campaign_name: Associated campaign name\n",
        "        location: Station location\n",
        "        sensor_types: List of sensor types at this station\n",
        "        owner_org: CKAN owner_org\n",
        "        ckan_url: CKAN instance URL\n",
        "        \n",
        "    Returns:\n",
        "        CKAN dataset information\n",
        "    \"\"\"\n",
        "    tags = []\n",
        "    if sensor_types:\n",
        "        tags.extend(sensor_types)\n",
        "    if campaign_name:\n",
        "        tags.append(f\"campaign-{campaign_name}\")\n",
        "    tags.extend([\"sensor-station\", \"environmental-data\", \"upstream\"])\n",
        "    # Enhanced description\n",
        "    enhanced_description = station_description\n",
        "    if campaign_name:\n",
        "        enhanced_description += f\"\\nCampaign: {campaign_name}\"\n",
        "    if sensor_types:\n",
        "        enhanced_description += f\"\\nSensor Types: {', '.join(sensor_types)}\"\n",
        "    # Step 3: Create CKAN dataset\n",
        "    print(\"3️⃣  Creating CKAN dataset...\")\n",
        "    dataset = create_ckan_dataset(\n",
        "        jwt_token=jwt_token,\n",
        "        dataset_name=station_name,\n",
        "        title=station_title,\n",
        "        description=enhanced_description,\n",
        "        tags=tags,\n",
        "        owner_org=owner_org,\n",
        "        ckan_url=ckan_url\n",
        "    )\n",
        "    print(\"✅ Station registration completed!\")\n",
        "    return dataset\n",
        "\n",
        "def add_resources_to_station(\n",
        "    jwt_token: str,\n",
        "    dataset_id: str,\n",
        "    resources: list,\n",
        "    ckan_url: str = \"https://ckan.tacc.utexas.edu\"\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Add data resources (files/URLs) to a station dataset.\n",
        "    Args:\n",
        "        jwt_token: JWT authentication token\n",
        "        dataset_id: CKAN dataset ID\n",
        "        resources: List of resource dictionaries\n",
        "        ckan_url: CKAN instance URL\n",
        "        \n",
        "    Returns:\n",
        "        List of created resources\n",
        "    \"\"\"\n",
        "    \n",
        "    api_url = f\"{ckan_url}/api/3/action/resource_create\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {jwt_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    created_resources = []\n",
        "    for resource in resources:\n",
        "        resource_data = {\n",
        "            \"package_id\": dataset_id,\n",
        "            **resource\n",
        "        }\n",
        "        print(f\"📎 Adding resource: {resource.get('name', 'Unnamed')}\")\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                api_url,\n",
        "                headers=headers,\n",
        "                json=resource_data\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "            if result.get(\"success\"):\n",
        "                created_resources.append(result[\"result\"])\n",
        "                print(f\"   ✅ Resource added: {result['result']['id']}\")\n",
        "            else:\n",
        "                print(f\"   ❌ Failed to add resource: {result}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Error adding resource: {e}\")\n",
        "    return created_resources\n",
        "\n",
        "# Load station metadata from JSON file\n",
        "def load_station_metadata(json_file_path: str = \"stations/station.json\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Load station metadata from JSON file.\n",
        "    \n",
        "    Args:\n",
        "        json_file_path: Path to the station JSON file\n",
        "        \n",
        "    Returns:\n",
        "        Station metadata dictionary\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(json_file_path, 'r') as f:\n",
        "            station_data = json.load(f)        \n",
        "        print(f\"📋 Loaded station metadata from {json_file_path}\")\n",
        "        print(f\"   Station: {station_data.get('name', 'Unknown')}\")\n",
        "        print(f\"   Project: {station_data.get('projectid', 'Unknown')}\")\n",
        "        print(f\"   Active: {station_data.get('active', 'Unknown')}\")\n",
        "        return station_data\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ Station metadata file not found: {json_file_path}\")\n",
        "        raise\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"❌ Invalid JSON in station file: {e}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading station metadata: {e}\")\n",
        "        raise\n",
        "\n",
        "def convert_station_metadata_for_ckan(station_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Convert station metadata to CKAN-compatible format.\n",
        "    \n",
        "    Args:\n",
        "        station_data: Raw station metadata from JSON\n",
        "        \n",
        "    Returns:\n",
        "        CKAN-compatible station information\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create CKAN-compatible dataset name (lowercase, no spaces, no special chars)\n",
        "    station_name = station_data.get('name', 'unknown-station')\n",
        "    ckan_name = station_name.lower().replace(' ', '-').replace('/', '-').replace('_', '-')\n",
        "    # Remove any remaining special characters\n",
        "    import re\n",
        "    ckan_name = re.sub(r'[^a-z0-9\\-]', '', ckan_name)\n",
        "    # Build enhanced description\n",
        "    description_parts = [station_data.get('description', 'Environmental monitoring station')]\n",
        "    if station_data.get('projectid'):\n",
        "        description_parts.append(f\"Project: {station_data['projectid']}\")\n",
        "    if station_data.get('contact_name'):\n",
        "        description_parts.append(f\"Contact: {station_data['contact_name']}\")\n",
        "        if station_data.get('contact_email'):\n",
        "            description_parts.append(f\"Email: {station_data['contact_email']}\")\n",
        "    if station_data.get('start_date'):\n",
        "        description_parts.append(f\"Start Date: {station_data['start_date']}\")\n",
        "    if station_data.get('active') is not None:\n",
        "        status = \"Active\" if station_data['active'] else \"Inactive\"\n",
        "        description_parts.append(f\"Status: {status}\")\n",
        "    enhanced_description = \"\\n\\n\".join(description_parts)\n",
        "    # Create tags from project and other metadata\n",
        "    tags = [\"environmental-monitoring\", \"upstream\", \"sensor-station\"]\n",
        "    if station_data.get('projectid'):\n",
        "        # Clean project ID for tag\n",
        "        project_tag = station_data['projectid'].lower().replace(' ', '-').replace('_', '-')\n",
        "        project_tag = re.sub(r'[^a-z0-9\\-]', '', project_tag)\n",
        "        tags.append(f\"project-{project_tag}\")\n",
        "    return( {\n",
        "        \"station_name\": ckan_name,\n",
        "        \"station_title\": station_data.get('name', 'Unknown Station'),\n",
        "        \"station_description\": enhanced_description,\n",
        "        \"campaign_name\": station_data.get('projectid'),\n",
        "        \"owner_org\":\"setx-uifl\",\n",
        "        \"author\":station_data.get('contact_name'),\n",
        "        \"author_email\":station_data.get('contact_email'),\n",
        "        \"sensor_types\": [\"water-level\", \"stream-gauge\"],  # Inferred from description\n",
        "        \"raw_metadata\": station_data  # Keep original data for reference\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ceeb4cf",
      "metadata": {},
      "source": [
        "## ⚙️ Running the Station Registration Workflow\n",
        "\n",
        "This section of the code provides **two options** for registering environmental monitoring stations to CKAN, based on your configuration files:\n",
        "\n",
        "### 🧪 Create a Single Station\n",
        "\n",
        "If you're working with **one station at a time**, this block reads a single configuration file (`stations/station.json`) and walks through the entire registration process:\n",
        "\n",
        "- Loads metadata  \n",
        "- Formats it for CKAN  \n",
        "- Registers the station as a dataset  \n",
        "- Returns a station ID upon success\n",
        "\n",
        "💡 *Useful when you're testing or onboarding new stations one by one.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "e3f0d1ed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Creating Single Station from Configuration ===\n",
            "📄 Loaded station config from: stations/station.json\n",
            "📋 Station Configuration Summary:\n",
            "  Name: Cow Bayou near Mauriceville\n",
            "  Project ID: SETx-UIFL Beaumont\n",
            "  Description: Beaumont Run stream gauge at Cow Bayou...\n",
            "  Contact: Nick Brake\n",
            "  Active: True\n",
            "  Start Date: 2025-06-02T14:42:00+0000\n",
            "✅ Station created successfully!\n",
            "Station ID: 39\n",
            "Station Name: Cow Bayou near Mauriceville\n",
            "Project ID: SETx-UIFL Beaumont\n",
            "Contact: Nick Brake\n",
            "\n",
            "🎉 Station setup complete!\n",
            "Station ID: 39\n",
            "\n",
            "==================================================\n",
            "=== Creating Multiple Stations from Configuration ===\n",
            "📋 Creating 2 station(s)...\n",
            "\n",
            "--- Creating Station 1/2 ---\n",
            "✅ Station created successfully!\n",
            "Station ID: 40\n",
            "Station Name: Cow Bayou near Mauriceville\n",
            "Project ID: SETx-UIFL Beaumont\n",
            "Contact: Nick Brake\n",
            "\n",
            "--- Creating Station 2/2 ---\n",
            "✅ Station created successfully!\n",
            "Station ID: 41\n",
            "Station Name: Pine Island Bayou near Sour Lake\n",
            "Project ID: SETx-UIFL Beaumont\n",
            "Contact: Nick Brake\n",
            "\n",
            "🎉 Created 2 station(s) successfully!\n",
            "  • Unknown (ID: 40)\n",
            "  • Unknown (ID: 41)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    station = load_and_create_station(\n",
        "        campaign_id=campaign_id,\n",
        "        config_path=\"stations/station.json\",\n",
        "        token=token\n",
        "    )    \n",
        "    station_id = station['id']\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ Configuration file error: {e}\")\n",
        "    print(\"💡 Please create a stations/station.json file with your station details\")\n",
        "except ValueError as e:\n",
        "    print(f\"❌ Configuration error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Station creation failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5f615a0",
      "metadata": {},
      "source": [
        "### 🧩 Create Multiple Stations\n",
        "\n",
        "Need to register **several stations at once**? This block processes a configuration file (`stations/stations.json`) containing a list of station definitions. It will:\n",
        "\n",
        "- Loop through each station entry  \n",
        "- Run the registration process for each  \n",
        "- Report success or failure for individual stations\n",
        "\n",
        "💡 *Great for batch imports or syncing an entire sensor network in one go.*\n",
        "\n",
        "Both workflows include helpful print statements and error handling to guide you through common issues — such as missing files or malformed configs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "9f9bb5e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    stations = load_and_create_multiple_stations(\n",
        "        campaign_id=campaign_id,\n",
        "        config_path=\"stations/stations.json\",\n",
        "        token=token\n",
        "    )    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ Configuration file error: {e}\")\n",
        "    print(\"💡 Please create a stations/stations.json file with your station details\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Multiple stations creation failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "117c47c1",
      "metadata": {},
      "source": [
        "# 🛰️ Station Registration & Resource Publishing Guide\n",
        "\n",
        "This document guides you through the registration of a station and the publication of its associated metadata and resources into a CKAN data portal.\n",
        "\n",
        "---\n",
        "\n",
        "## 1️⃣ Load Station Metadata\n",
        "\n",
        "Begin by loading your station's configuration from a local JSON file.\n",
        "\n",
        "- **File:** `./stations/station.json`\n",
        "- **Expected Fields:**\n",
        "  - `name`\n",
        "  - `projectid`\n",
        "  - `contact_name`\n",
        "  - `contact_email`\n",
        "  - `start_date`\n",
        "  - ...and other relevant metadata\n",
        "\n",
        "---\n",
        "\n",
        "## 2️⃣ Convert Metadata to CKAN Format\n",
        "\n",
        "Transform the raw station metadata into the format expected by CKAN. This typically includes:\n",
        "\n",
        "- `station_name`: A machine-readable slug (e.g., `lake-travis-buoy`)\n",
        "- `station_title`: A human-readable title\n",
        "- `campaign_name`: Associated research campaign\n",
        "- `tags`, `groups`, bounding boxes, and other CKAN-compatible fields\n",
        "\n",
        "---\n",
        "\n",
        "## 3️⃣ Register Station in CKAN\n",
        "\n",
        "Use your Tapis JWT token to register the station with CKAN.\n",
        "\n",
        "- ✅ **Dataset ID**\n",
        "- ✅ **Dataset Name**\n",
        "- ✅ **CKAN URL**  \n",
        "  Format: `https://ckan.tacc.utexas.edu/dataset/<dataset-name>`\n",
        "\n",
        "---\n",
        "\n",
        "## 4️⃣ Add Station Resources\n",
        "\n",
        "Add data endpoints and visualizations as resources to enrich the dataset.\n",
        "\n",
        "### 🔗 Base Resources\n",
        "\n",
        "- **Station Information**  \n",
        "  > Full metadata & configuration for this station  \n",
        "  `JSON` - `/api/v1/campaigns/<campaign_id>/stations/<station_id>`\n",
        "\n",
        "- **All Station Sensors**  \n",
        "  > List of all sensors deployed at the station  \n",
        "  `JSON` - `/api/v1/campaigns/<campaign_id>/stations/<station_id>/sensors`\n",
        "\n",
        "- **All Sensors and Visualizations**  \n",
        "  > Frontend dashboard for sensors and charts  \n",
        "  `Website` - `https://dso-tacc.netlify.app/campaigns/<campaign_id>/stations/<station_id>`\n",
        "\n",
        "- **Aggregated Statistics**  \n",
        "  > Time-aggregated measurements with statistical analysis  \n",
        "  `JSON` - `/api/v1/campaigns/<campaign_id>/stations/<station_id>/measurements/aggregated`\n",
        "\n",
        "---\n",
        "\n",
        "### 🧾 Optional: Contact Information\n",
        "\n",
        "If `contact_name` or `contact_email` is provided in the JSON, a text-based resource is added:\n",
        "\n",
        "**Contact Information**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45d89de8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1️⃣  Loading station metadata from JSON...\n",
            "📋 Loaded station metadata from ./stations/station.json\n",
            "   Station: Cow Bayou near Mauriceville\n",
            "   Project: SETx-UIFL Beaumont\n",
            "   Active: True\n",
            "2️⃣  Converting metadata for CKAN...\n",
            "   CKAN Dataset Name: cow-bayou-near-mauriceville\n",
            "   Title: Cow Bayou near Mauriceville\n",
            "   Campaign: SETx-UIFL Beaumont\n",
            "3️⃣  Registering station to CKAN...\n",
            "============================================================\n",
            "🚀 REGISTERING STATION TO CKAN\n",
            "============================================================\n",
            "3️⃣  Creating CKAN dataset...\n",
            "🏗️  Creating CKAN dataset: cow-bayou-near-mauriceville\n",
            "   Title: Cow Bayou near Mauriceville\n",
            "   URL: https://ckan.tacc.utexas.edu/api/3/action/package_create\n",
            "✅ Dataset created successfully!\n",
            "   Dataset ID: e6c2acb7-99a4-44ad-9935-d220874dd10c\n",
            "   Dataset URL: https://ckan.tacc.utexas.edu/dataset/cow-bayou-near-mauriceville\n",
            "✅ Station registration completed!\n",
            "\n",
            "🎉 Station registered successfully!\n",
            "Dataset ID: e6c2acb7-99a4-44ad-9935-d220874dd10c\n",
            "Dataset Name: cow-bayou-near-mauriceville\n",
            "Dataset URL: https://ckan.tacc.utexas.edu/dataset/cow-bayou-near-mauriceville\n",
            "\n",
            "4️⃣  Adding data resources...\n",
            "📎 Adding resource: Station Information\n",
            "   ✅ Resource added: e4d9fe1e-7049-498c-945e-5ddfaf1f59e7\n",
            "📎 Adding resource: All Station Sensors\n",
            "   ✅ Resource added: a085396d-e08b-40ef-ae11-d637c34cf64e\n",
            "📎 Adding resource: All Sensors and Visualizations\n",
            "   ✅ Resource added: 81fb74f5-8b16-4dc2-abe5-5f78946dc33f\n",
            "📎 Adding resource: Aggregated Statistics\n",
            "   ✅ Resource added: 6d9b705d-f8f4-4ba7-bc41-1aed5da88c6d\n",
            "📎 Adding resource: Contact Information\n",
            "   ✅ Resource added: 0a6721be-f56f-47b1-bd13-ba40c914a188\n",
            "\n",
            "📊 Added 5 resources to station\n",
            "\n",
            "============================================================\n",
            "📋 REGISTRATION SUMMARY\n",
            "============================================================\n",
            "Station Name: Cow Bayou near Mauriceville\n",
            "Project: SETx-UIFL Beaumont\n",
            "CKAN Dataset: cow-bayou-near-mauriceville\n",
            "Dataset URL: https://ckan.tacc.utexas.edu/dataset/cow-bayou-near-mauriceville\n",
            "Contact: Nick Brake\n",
            "Status: Active\n",
            "Resources Added: 5\n"
          ]
        }
      ],
      "source": [
        "# Load station metadata from JSON file\n",
        "raw_station_data = load_station_metadata(\"./stations/station.json\")\n",
        "\n",
        "# Convert to CKAN format\n",
        "station_info = convert_station_metadata_for_ckan(raw_station_data)\n",
        "\n",
        "# Register the station\n",
        "dataset = register_station_to_ckan(\n",
        "jwt_token=tapis_token.access_token,\n",
        "    **{k: v for k, v in station_info.items() if k != 'raw_metadata'}\n",
        ")\n",
        "\n",
        "# Get JWT token again (in case it expired)\n",
        "jwt_token = tapis_token.access_token\n",
        "station_name = raw_station_data.get('name', 'this station')\n",
        "base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
        "\n",
        "# Base station resources\n",
        "resources = [\n",
        "        {\n",
        "            \"name\": \"Station Information\",\n",
        "            \"description\": f\"Complete station metadata and configuration for {station_name}\",\n",
        "            \"format\": \"JSON\",\n",
        "            \"url\": f\"{base_url}/api/v1/campaigns/{campaign_id}/stations/{station_id}\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"All Station Sensors\",\n",
        "            \"description\": f\"Complete list of sensors deployed at {station_name}\",\n",
        "            \"format\": \"JSON\",\n",
        "            \"url\": f\"{base_url}/api/v1/campaigns/{campaign_id}/stations/{station_id}/sensors\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"All Sensors and Visualizations\",\n",
        "            \"description\": f\"All Sensors and Visualizations from {station_name} (paginated)\",\n",
        "            \"format\": \"Website\",\n",
        "            \"url\": f\"https://dso-tacc.netlify.app/campaigns/{campaign_id}/stations/{station_id}\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Aggregated Statistics\",\n",
        "            \"description\": f\"Time-aggregated measurements with statistical analysis from {station_name}\",\n",
        "            \"format\": \"JSON\",\n",
        "            \"url\": f\"{base_url}/api/v1/campaigns/{campaign_id}/stations/{station_id}/measurements/aggregated\"\n",
        "        }\n",
        "    ]\n",
        "# Add contact information as a resource if available\n",
        "if raw_station_data.get('contact_name') or raw_station_data.get('contact_email'):\n",
        "    contact_info = {\n",
        "        \"name\": \"Contact Information\",\n",
        "        \"description\": \"Station contact and project information\",\n",
        "        \"format\": \"TEXT\"\n",
        "    }\n",
        "    contact_text = f\"Station Contact Information\\n\"\n",
        "    contact_text += f\"Station: {raw_station_data.get('name', 'Unknown')}\\n\"\n",
        "    contact_text += f\"Project: {raw_station_data.get('projectid', 'Unknown')}\\n\"\n",
        "    if raw_station_data.get('contact_name'):\n",
        "        contact_text += f\"Contact Name: {raw_station_data['contact_name']}\\n\"\n",
        "    if raw_station_data.get('contact_email'):\n",
        "        contact_text += f\"Contact Email: {raw_station_data['contact_email']}\\n\"\n",
        "    if raw_station_data.get('start_date'):\n",
        "        contact_text += f\"Start Date: {raw_station_data['start_date']}\\n\"\n",
        "    # For this example, we'll add it as a URL (you might want to upload as a file instead)\n",
        "    resources.append(contact_info)\n",
        "\n",
        "created_resources = add_resources_to_station(\n",
        "    jwt_token=jwt_token,\n",
        "    dataset_id=dataset['id'],\n",
        "    resources=resources\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "593ba293",
      "metadata": {},
      "source": [
        "## CSV Data Upload Function Documentation\n",
        "\n",
        "### Overview\n",
        "\n",
        "The `upload_csv_data` function provides a streamlined way to upload sensor and measurement data to the Upstream platform via CSV files. This function handles file validation, authentication, and provides detailed feedback on the upload process.\n",
        "\n",
        "\n",
        "### Parameters\n",
        "\n",
        "| Parameter | Type | Required | Description |\n",
        "|-----------|------|----------|-------------|\n",
        "| `campaign_id` | `int` | ✅ | Unique identifier for the target campaign |\n",
        "| `station_id` | `int` | ✅ | Unique identifier for the target station within the campaign |\n",
        "| `sensors_file_path` | `str` | ✅ | Local file path to the sensors CSV file |\n",
        "| `measurements_file_path` | `str` | ✅ | Local file path to the measurements CSV file |\n",
        "| `token` | `str` | ✅ | Authentication token for API access |\n",
        "| `base_url` | `str` | ❌ | Base URL for the Upstream API (defaults to dev environment) |\n",
        "\n",
        "### Return Value\n",
        "\n",
        "Returns a `Dict[str, Any]` containing the upload response data with statistics including:\n",
        "- Total sensors processed\n",
        "- Total measurements added to database\n",
        "- Data processing time\n",
        "\n",
        "### Features\n",
        "\n",
        "#### 🔍 **File Validation**\n",
        "- Automatically checks if both CSV files exist before attempting upload\n",
        "- Raises `FileNotFoundError` with descriptive messages for missing files\n",
        "\n",
        "#### 📊 **Progress Tracking**\n",
        "- Displays upload parameters for verification\n",
        "- Shows real-time upload status with emoji indicators\n",
        "- Provides detailed statistics upon completion\n",
        "\n",
        "#### 🔐 **Secure Upload**\n",
        "- Uses authenticated requests via the `make_authenticated_request` helper\n",
        "- Properly formats files for multipart form data upload\n",
        "\n",
        "#### 🎯 **Error Handling**\n",
        "- Pre-upload file existence validation\n",
        "- Clear error messages for troubleshooting\n",
        "\n",
        "### API Endpoint\n",
        "\n",
        "The function uploads to the following endpoint:\n",
        "```\n",
        "POST {base_url}/api/v1/uploadfile_csv/campaign/{campaign_id}/station/{station_id}/sensor\n",
        "```\n",
        "\n",
        "### File Format Requirements\n",
        "\n",
        "#### Sensors CSV\n",
        "- Must contain sensor definition data\n",
        "- Uploaded as `upload_file_sensors` form field\n",
        "\n",
        "#### Measurements CSV  \n",
        "- Must contain measurement data corresponding to the sensors\n",
        "- Uploaded as `upload_file_measurements` form field\n",
        "\n",
        "### Console Output Example\n",
        "\n",
        "```\n",
        "=== Uploading CSV Data ===\n",
        "Campaign ID: 123\n",
        "Station ID: 456\n",
        "Sensors file: ./data/sensors.csv\n",
        "Measurements file: ./data/measurements.csv\n",
        "📤 Uploading files...\n",
        "✅ Upload completed successfully!\n",
        "📊 Upload Statistics:\n",
        "  • Sensors processed: 15\n",
        "  • Measurements added: 1,250\n",
        "  • Processing time: 2.3s\n",
        "```\n",
        "\n",
        "### Dependencies\n",
        "\n",
        "- `os` - For file existence checking\n",
        "- `make_authenticated_request` - Custom function for authenticated API calls\n",
        "- `Dict`, `Any` from `typing` - For type hints\n",
        "\n",
        "### Error Scenarios\n",
        "\n",
        "| Error Type | Cause | Solution |\n",
        "|------------|-------|----------|\n",
        "| `FileNotFoundError` | CSV file doesn't exist at specified path | Verify file paths are correct |\n",
        "| Authentication errors | Invalid or expired token | Refresh authentication token |\n",
        "| API errors | Server issues or invalid parameters | Check campaign/station IDs and API status |\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Validate Data First**: Ensure your CSV files are properly formatted before upload\n",
        "2. **Check Permissions**: Verify you have write access to the specified campaign/station\n",
        "3. **Monitor Output**: Pay attention to the upload statistics to confirm expected data volumes\n",
        "4. **Handle Errors**: Always wrap calls in try-catch blocks for production use\n",
        "5. **Use Absolute Paths**: Prefer absolute file paths to avoid path resolution issues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee3ba22-8018-4d8d-86ed-04fb62ebc6b1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Listing Available Data Files ===\n",
            "📁 Files found in ./data/:\n",
            "  • Total CSV files: 2\n",
            "  • Sensor files: 1\n",
            "  • Measurement files: 1\n",
            "📄 All CSV files:\n",
            "    - measurements.csv (906,949 bytes)\n",
            "    - sensors.csv (173 bytes)\n",
            "\n",
            "==================================================\n",
            "=== Uploading CSV Data (Standard Method) ===\n",
            "=== Uploading CSV Data ===\n",
            "Campaign ID: 12\n",
            "Station ID: 39\n",
            "Data Directory: ./data/\n",
            "Sensors file: ./data/sensors.csv\n",
            "Measurements file: ./data/measurements.csv\n",
            "📁 File Information:\n",
            "  • Sensors file size: 173 bytes\n",
            "  • Measurements file size: 906,949 bytes\n",
            "📤 Uploading files...\n",
            "✅ Upload completed successfully!\n",
            "📊 Upload Statistics:\n",
            "  • Sensors processed: 3\n",
            "  • Measurements added: 26881\n",
            "  • Processing time: 9.2 seconds.\n",
            "\n",
            "🎉 Data upload complete!\n",
            "\n",
            "==================================================\n",
            "=== Uploading CSV Data (Auto-Detection Method) ===\n",
            "=== Auto-detecting Data Files ===\n",
            "📁 Files found in ./data/:\n",
            "  • Total CSV files: 2\n",
            "  • Sensor files: 1\n",
            "  • Measurement files: 1\n",
            "📄 All CSV files:\n",
            "    - measurements.csv (906,949 bytes)\n",
            "    - sensors.csv (173 bytes)\n",
            "=== Uploading CSV Data ===\n",
            "Campaign ID: 12\n",
            "Station ID: 39\n",
            "Data Directory: ./data/\n",
            "Sensors file: ./data/sensors.csv\n",
            "Measurements file: ./data/measurements.csv\n",
            "📁 File Information:\n",
            "  • Sensors file size: 173 bytes\n",
            "  • Measurements file size: 906,949 bytes\n",
            "📤 Uploading files...\n",
            "✅ Upload completed successfully!\n",
            "📊 Upload Statistics:\n",
            "  • Sensors processed: 3\n",
            "  • Measurements added: 0\n",
            "  • Processing time: 8.3 seconds.\n",
            "\n",
            "🎉 Auto-detected data upload complete!\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def upload_csv_data(\n",
        "    campaign_id: int,\n",
        "    station_id: int,\n",
        "    token: str,\n",
        "    data_dir: str = \"./data/\",\n",
        "    author:str=None,\n",
        "    author_email:str=None,\n",
        "    sensors_filename: str = \"sensors.csv\",\n",
        "    measurements_filename: str = \"measurements.csv\",\n",
        "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Upload sensor and measurement CSV files to Upstream from data directory.\n",
        "    \n",
        "    Args:\n",
        "        campaign_id: ID of the target campaign\n",
        "        station_id: ID of the target station\n",
        "        token: Access token\n",
        "        data_dir: Directory containing CSV files (default: \"./data/\")\n",
        "        sensors_filename: Name of sensors CSV file (default: \"sensors.csv\")\n",
        "        measurements_filename: Name of measurements CSV file (default: \"measurements.csv\")\n",
        "        base_url: Base URL for the API\n",
        "        \n",
        "    Returns:\n",
        "        Upload response data\n",
        "    \"\"\"\n",
        "    # Construct file paths\n",
        "    sensors_file_path = os.path.join(data_dir, sensors_filename)\n",
        "    measurements_file_path = os.path.join(data_dir, measurements_filename)    \n",
        "    upload_url = f\"{base_url}/api/v1/uploadfile_csv/campaign/{campaign_id}/station/{station_id}/sensor\"\n",
        "    print(f\"=== Uploading CSV Data ===\")\n",
        "    print(f\"Campaign ID: {campaign_id}\")\n",
        "    print(f\"Station ID: {station_id}\")\n",
        "    print(f\"Data Directory: {data_dir}\")\n",
        "    print(f\"Sensors file: {sensors_file_path}\")\n",
        "    print(f\"Measurements file: {measurements_file_path}\")\n",
        "    # Verify files exist\n",
        "    if not os.path.exists(sensors_file_path):\n",
        "        raise FileNotFoundError(f\"Sensors file not found: {sensors_file_path}\")\n",
        "    if not os.path.exists(measurements_file_path):\n",
        "        raise FileNotFoundError(f\"Measurements file not found: {measurements_file_path}\")\n",
        "    # Display file information\n",
        "    sensors_size = os.path.getsize(sensors_file_path)\n",
        "    measurements_size = os.path.getsize(measurements_file_path)\n",
        "    print(f\"📁 File Information:\")\n",
        "    print(f\"  • Sensors file size: {sensors_size:,} bytes\")\n",
        "    print(f\"  • Measurements file size: {measurements_size:,} bytes\")\n",
        "    \n",
        "    # Prepare files for upload\n",
        "    with open(sensors_file_path, 'rb') as sensors_file, \\\n",
        "            open(measurements_file_path, 'rb') as measurements_file:\n",
        "        files = {\n",
        "            'upload_file_sensors': (sensors_filename, sensors_file, 'text/csv'),\n",
        "            'upload_file_measurements': (measurements_filename, measurements_file, 'text/csv')\n",
        "        }\n",
        "        print(\"📤 Uploading files...\")\n",
        "        response = make_authenticated_request(\n",
        "            method=\"POST\",\n",
        "            url=upload_url,\n",
        "            token=token,\n",
        "            files=files\n",
        "        )\n",
        "        result = response.json()\n",
        "        print(\"✅ Upload completed successfully!\")\n",
        "        # Display upload statistics\n",
        "        print(f\"📊 Upload Statistics:\")\n",
        "        print(f\"  • Sensors processed: {result.get('Total sensors processed', 'N/A')}\")\n",
        "        print(f\"  • Measurements added: {result.get('Total measurements added to database', 'N/A')}\")\n",
        "        print(f\"  • Processing time: {result.get('Data Processing time', 'N/A')}\")\n",
        "        return result\n",
        "\n",
        "def list_data_files(data_dir: str = \"./data/\") -> Dict[str, list]:\n",
        "    \"\"\"\n",
        "    List all CSV files in the data directory.\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Directory to search for CSV files\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with lists of found files\n",
        "    \"\"\"\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"❌ Data directory not found: {data_dir}\")\n",
        "        return {\"csv_files\": [], \"sensors_files\": [], \"measurements_files\": []}\n",
        "    \n",
        "    # Find all CSV files\n",
        "    csv_pattern = os.path.join(data_dir, \"*.csv\")\n",
        "    csv_files = glob.glob(csv_pattern)\n",
        "    \n",
        "    # Categorize files\n",
        "    sensors_files = [f for f in csv_files if 'sensor' in os.path.basename(f).lower()]\n",
        "    measurements_files = [f for f in csv_files if 'measurement' in os.path.basename(f).lower()]\n",
        "    \n",
        "    print(f\"📁 Files found in {data_dir}:\")\n",
        "    print(f\"  • Total CSV files: {len(csv_files)}\")\n",
        "    print(f\"  • Sensor files: {len(sensors_files)}\")\n",
        "    print(f\"  • Measurement files: {len(measurements_files)}\")\n",
        "    \n",
        "    if csv_files:\n",
        "        print(f\"📄 All CSV files:\")\n",
        "        for file in csv_files:\n",
        "            size = os.path.getsize(file)\n",
        "            print(f\"    - {os.path.basename(file)} ({size:,} bytes)\")\n",
        "    \n",
        "    return {\n",
        "        \"csv_files\": csv_files,\n",
        "        \"sensors_files\": sensors_files,\n",
        "        \"measurements_files\": measurements_files\n",
        "    }\n",
        "\n",
        "def upload_data_with_auto_detection(\n",
        "    campaign_id: int,\n",
        "    station_id: int,\n",
        "    token: str,\n",
        "    data_dir: str = \"./data/\",\n",
        "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Upload CSV data with automatic file detection.\n",
        "    \n",
        "    Args:\n",
        "        campaign_id: ID of the target campaign\n",
        "        station_id: ID of the target station\n",
        "        token: Access token\n",
        "        data_dir: Directory containing CSV files\n",
        "        base_url: Base URL for the API\n",
        "        \n",
        "    Returns:\n",
        "        Upload response data\n",
        "    \"\"\"\n",
        "    print(\"=== Auto-detecting Data Files ===\")\n",
        "    files_info = list_data_files(data_dir)\n",
        "    # Try to find sensors and measurements files\n",
        "    sensors_file = None\n",
        "    measurements_file = None\n",
        "    # Look for standard filenames first\n",
        "    standard_sensors = os.path.join(data_dir, \"sensors.csv\")\n",
        "    standard_measurements = os.path.join(data_dir, \"measurements.csv\")\n",
        "    if os.path.exists(standard_sensors):\n",
        "        sensors_file = \"sensors.csv\"\n",
        "    elif files_info[\"sensors_files\"]:\n",
        "        sensors_file = os.path.basename(files_info[\"sensors_files\"][0])\n",
        "        print(f\"🔍 Using detected sensors file: {sensors_file}\")\n",
        "    if os.path.exists(standard_measurements):\n",
        "        measurements_file = \"measurements.csv\"\n",
        "    elif files_info[\"measurements_files\"]:\n",
        "        measurements_file = os.path.basename(files_info[\"measurements_files\"][0])\n",
        "        print(f\"🔍 Using detected measurements file: {measurements_file}\")\n",
        "    \n",
        "    if not sensors_file or not measurements_file:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not find required files. \"\n",
        "            f\"Sensors: {sensors_file}, Measurements: {measurements_file}\"\n",
        "        )\n",
        "    \n",
        "    # Upload the files\n",
        "    return upload_csv_data(\n",
        "        campaign_id=campaign_id,\n",
        "        station_id=station_id,\n",
        "        token=token,\n",
        "        data_dir=data_dir,\n",
        "        sensors_filename=sensors_file,\n",
        "        measurements_filename=measurements_file,\n",
        "        base_url=base_url\n",
        "    )\n",
        "\n",
        "# Usage examples\n",
        "data_files = list_data_files(\"./data/\")\n",
        "try:\n",
        "    # Upload using standard filenames\n",
        "    result = upload_csv_data(\n",
        "        campaign_id=campaign_id,\n",
        "        station_id=station_id,\n",
        "        token=token,\n",
        "        data_dir=\"./data/\",\n",
        "        sensors_filename=\"sensors.csv\",\n",
        "        measurements_filename=\"measurements.csv\"\n",
        "    )\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ File error: {e}\")\n",
        "    print(\"💡 Make sure your CSV files are in the ./data/ directory\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Upload failed: {e}\")\n",
        "try:\n",
        "    # Upload with automatic file detection\n",
        "    result = upload_data_with_auto_detection(\n",
        "        campaign_id=campaign_id,\n",
        "        station_id=station_id,\n",
        "        token=token,\n",
        "        data_dir=\"./data/\"\n",
        "    )    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ File detection error: {e}\")\n",
        "    print(\"💡 Make sure your CSV files are in the ./data/ directory with 'sensor' and 'measurement' in their names\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Auto-upload failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88c765ce",
      "metadata": {},
      "source": [
        "### CSV File Format Examples\n",
        "\n",
        "#### Sensors CSV Format\n",
        "\n",
        "Your `sensors.csv` file defines the sensor metadata and should follow this structure:\n",
        "\n",
        "```csv\n",
        "alias,variablename,units,postprocess,postprocessscript\n",
        "temp_sensor_01,Air Temperature,°C,,\n",
        "humidity_01,Relative Humidity,%,,\n",
        "pressure_01,Atmospheric Pressure,hPa,,\n",
        "wind_speed_01,Wind Speed,m/s,true,wind_correction_script\n",
        "```\n",
        "\n",
        "**Column Descriptions:**\n",
        "- `alias`: Unique identifier for the sensor (used as column header in measurements)\n",
        "- `variablename`: Human-readable description of what the sensor measures\n",
        "- `units`: Measurement units (e.g., °C, %, hPa, m/s)\n",
        "- `postprocess`: Boolean flag indicating if post-processing is required\n",
        "- `postprocessscript`: Name of the post-processing script (if applicable)\n",
        "\n",
        "#### Measurements CSV Format\n",
        "\n",
        "Your `measurements.csv` file contains the actual sensor data and should follow this structure:\n",
        "\n",
        "```csv\n",
        "collectiontime,Lat_deg,Lon_deg,temp_sensor_01,humidity_01,pressure_01,wind_speed_01\n",
        "2024-01-15T10:30:00,30.2672,-97.7431,23.5,65.2,1013.25,2.3\n",
        "2024-01-15T10:31:00,30.2673,-97.7432,23.7,64.8,1013.20,2.1\n",
        "2024-01-15T10:32:00,30.2674,-97.7433,23.9,64.5,1013.15,1.8\n",
        "2024-01-15T10:33:00,30.2675,-97.7434,,64.2,1013.10,1.9\n",
        "```\n",
        "\n",
        "**Required Columns:**\n",
        "- `collectiontime`: Timestamp in ISO 8601 format (YYYY-MM-DDTHH:MM:SS)\n",
        "- `Lat_deg`: Latitude in decimal degrees\n",
        "- `Lon_deg`: Longitude in decimal degrees\n",
        "\n",
        "**Sensor Data Columns:**\n",
        "- Each sensor `alias` from sensors.csv becomes a column header\n",
        "- Column names must exactly match the sensor aliases\n",
        "- Empty values are automatically handled (see row 4 in example)\n",
        "\n",
        "#### Important File Format Notes\n",
        "\n",
        "⚠️ **Critical Requirements:**\n",
        "- Each sensor `alias` from sensors.csv becomes a column in measurements.csv\n",
        "- `collectiontime`, `Lat_deg`, and `Lon_deg` are required columns in measurements.csv\n",
        "- Empty values are handled automatically by the system\n",
        "- Maximum file size is **500 MB per file**\n",
        "- Use UTF-8 encoding for both files\n",
        "- Timestamps should be in UTC or include timezone information\n",
        "\n",
        "📝 **Best Practices:**\n",
        "- Keep sensor aliases short but descriptive\n",
        "- Use consistent naming conventions (e.g., `sensor_type_number`)\n",
        "- Ensure measurement values match the units specified in sensors.csv\n",
        "- Include all sensors in measurements.csv even if some readings are missing\n",
        "\n",
        "\n",
        "#### Helper Function Features\n",
        "\n",
        "🔍 **Campaign Discovery:**\n",
        "- List all campaigns you have access to\n",
        "- View campaign metadata and descriptions\n",
        "- Identify the correct campaign ID for your data\n",
        "\n",
        "🏗️ **Station Management:**\n",
        "- List all stations within a campaign\n",
        "- View station details and locations\n",
        "- Find the appropriate station ID for your sensors\n",
        "\n",
        "💡 **Integration Tip:**\n",
        "Use these helper functions before uploading data to ensure you're targeting the correct campaign and station IDs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "062450c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Optional\n",
        "import time\n",
        "import math\n",
        "\n",
        "def get_file_info(file_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Get detailed information about a CSV file.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        return {}\n",
        "    \n",
        "    file_size = os.path.getsize(file_path)    \n",
        "    # Count rows efficiently\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        row_count = sum(1 for line in f) - 1  # Subtract header row\n",
        "    return {\n",
        "        'size_bytes': file_size,\n",
        "        'size_mb': file_size / (1024 * 1024),\n",
        "        'row_count': row_count,\n",
        "        'estimated_chunk_count': lambda chunk_size: math.ceil(row_count / chunk_size)\n",
        "    }\n",
        "\n",
        "def create_csv_chunks(\n",
        "    file_path: str,\n",
        "    chunk_size: int = 10000,\n",
        "    output_dir: Optional[str] = None,\n",
        "    max_file_size_mb: int = 50\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Split a large CSV file into smaller chunks.\n",
        "    \n",
        "    Args:\n",
        "        file_path: Path to the large CSV file\n",
        "        chunk_size: Number of rows per chunk\n",
        "        output_dir: Directory to store chunk files (temp dir if None)\n",
        "        max_file_size_mb: Maximum file size per chunk in MB\n",
        "        \n",
        "    Returns:\n",
        "        List of chunk file paths\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "    \n",
        "    # Create output directory\n",
        "    if output_dir is None:\n",
        "        output_dir = tempfile.mkdtemp(prefix=\"csv_chunks_\")\n",
        "    else:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    file_info = get_file_info(file_path)\n",
        "    filename = os.path.basename(file_path)\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    \n",
        "    print(f\"📦 Chunking {filename}:\")\n",
        "    print(f\"  • Total rows: {file_info['row_count']:,}\")\n",
        "    print(f\"  • File size: {file_info['size_mb']:.2f} MB\")\n",
        "    print(f\"  • Chunk size: {chunk_size:,} rows\")\n",
        "    print(f\"  • Estimated chunks: {file_info['estimated_chunk_count'](chunk_size)}\")\n",
        "    \n",
        "    chunk_files = []\n",
        "    try:\n",
        "        # Read and chunk the CSV file\n",
        "        chunk_num = 0\n",
        "        for chunk_df in pd.read_csv(file_path, chunksize=chunk_size):\n",
        "            chunk_num += 1\n",
        "            chunk_filename = f\"{name}_chunk_{chunk_num:03d}{ext}\"\n",
        "            chunk_path = os.path.join(output_dir, chunk_filename)\n",
        "            # Save chunk\n",
        "            chunk_df.to_csv(chunk_path, index=False)\n",
        "            # Check file size\n",
        "            chunk_size_mb = os.path.getsize(chunk_path) / (1024 * 1024)\n",
        "            if chunk_size_mb > max_file_size_mb:\n",
        "                print(f\"⚠️  Warning: Chunk {chunk_num} is {chunk_size_mb:.2f} MB (exceeds {max_file_size_mb} MB limit)\")\n",
        "            chunk_files.append(chunk_path)\n",
        "            print(f\"  ✓ Created chunk {chunk_num}: {len(chunk_df)} rows, {chunk_size_mb:.2f} MB\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        # Clean up on error\n",
        "        for chunk_file in chunk_files:\n",
        "            if os.path.exists(chunk_file):\n",
        "                os.remove(chunk_file)\n",
        "        raise e\n",
        "    \n",
        "    print(f\"📦 Created {len(chunk_files)} chunks in {output_dir}\")\n",
        "    return chunk_files\n",
        "\n",
        "def upload_csv_data_chunked(\n",
        "    campaign_id: int,\n",
        "    station_id: int,\n",
        "    token: str,\n",
        "    data_dir: str = \"./data/\",\n",
        "    sensors_filename: str = \"sensors.csv\",\n",
        "    measurements_filename: str = \"measurements.csv\",\n",
        "    chunk_size: int = 10000,\n",
        "    max_file_size_mb: int = 50,\n",
        "    cleanup_chunks: bool = True,\n",
        "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Upload large CSV files using chunking strategy.\n",
        "    \n",
        "    Args:\n",
        "        campaign_id: ID of the target campaign\n",
        "        station_id: ID of the target station\n",
        "        token: Access token\n",
        "        data_dir: Directory containing CSV files\n",
        "        sensors_filename: Name of sensors CSV file\n",
        "        measurements_filename: Name of measurements CSV file\n",
        "        chunk_size: Number of rows per chunk\n",
        "        max_file_size_mb: Maximum file size per chunk in MB\n",
        "        cleanup_chunks: Whether to delete chunk files after upload\n",
        "        base_url: Base URL for the API\n",
        "        \n",
        "    Returns:\n",
        "        Aggregated upload response data\n",
        "    \"\"\"\n",
        "    print(f\"=== Chunked CSV Data Upload ===\")\n",
        "    print(f\"Campaign ID: {campaign_id}\")\n",
        "    print(f\"Station ID: {station_id}\")\n",
        "    print(f\"Chunk size: {chunk_size:,} rows\")\n",
        "    print(f\"Max chunk file size: {max_file_size_mb} MB\")\n",
        "    \n",
        "    # Construct file paths\n",
        "    sensors_file_path = os.path.join(data_dir, sensors_filename)\n",
        "    measurements_file_path = os.path.join(data_dir, measurements_filename)\n",
        "    \n",
        "    # Verify files exist\n",
        "    if not os.path.exists(sensors_file_path):\n",
        "        raise FileNotFoundError(f\"Sensors file not found: {sensors_file_path}\")\n",
        "    if not os.path.exists(measurements_file_path):\n",
        "        raise FileNotFoundError(f\"Measurements file not found: {measurements_file_path}\")\n",
        "    \n",
        "    # Get file information\n",
        "    sensors_info = get_file_info(sensors_file_path)\n",
        "    measurements_info = get_file_info(measurements_file_path)\n",
        "    print(f\"\\n📁 File Analysis:\")\n",
        "    print(f\"  • Sensors: {sensors_info['row_count']:,} rows, {sensors_info['size_mb']:.2f} MB\")\n",
        "    print(f\"  • Measurements: {measurements_info['row_count']:,} rows, {measurements_info['size_mb']:.2f} MB\")\n",
        "    # Create temporary directory for chunks\n",
        "    chunk_dir = tempfile.mkdtemp(prefix=\"upload_chunks_\")\n",
        "    try:\n",
        "        # Create chunks\n",
        "        print(\"\\n--- Chunking Sensors File ---\")\n",
        "        sensors_chunks = create_csv_chunks(\n",
        "            sensors_file_path, \n",
        "            chunk_size=chunk_size,\n",
        "            output_dir=os.path.join(chunk_dir, \"sensors\"),\n",
        "            max_file_size_mb=max_file_size_mb\n",
        "        )\n",
        "        \n",
        "        print(\"\\n--- Chunking Measurements File ---\")\n",
        "        measurements_chunks = create_csv_chunks(\n",
        "            measurements_file_path,\n",
        "            chunk_size=chunk_size, \n",
        "            output_dir=os.path.join(chunk_dir, \"measurements\"),\n",
        "            max_file_size_mb=max_file_size_mb\n",
        "        )\n",
        "        \n",
        "        # Upload chunks\n",
        "        total_chunks = max(len(sensors_chunks), len(measurements_chunks))\n",
        "        successful_uploads = 0\n",
        "        failed_uploads = 0\n",
        "        aggregated_results = {\n",
        "            'total_sensors_processed': 0,\n",
        "            'total_measurements_added': 0,\n",
        "            'total_processing_time': 0,\n",
        "            'chunk_results': []\n",
        "        }\n",
        "        print(f\"\\n📤 Uploading {total_chunks} chunk pairs...\")\n",
        "        \n",
        "        for i in range(total_chunks):\n",
        "            chunk_num = i + 1\n",
        "            print(f\"\\n--- Uploading Chunk {chunk_num}/{total_chunks} ---\")   \n",
        "            try:\n",
        "                # Get chunk files (use last chunk if one file has fewer chunks)\n",
        "                sensors_chunk = sensors_chunks[min(i, len(sensors_chunks) - 1)]\n",
        "                measurements_chunk = measurements_chunks[min(i, len(measurements_chunks) - 1)]\n",
        "                # Upload chunk pair\n",
        "                start_time = time.time()\n",
        "                with open(sensors_chunk, 'rb') as sf, open(measurements_chunk, 'rb') as mf:\n",
        "                    files = {\n",
        "                        'upload_file_sensors': (os.path.basename(sensors_chunk), sf, 'text/csv'),\n",
        "                        'upload_file_measurements': (os.path.basename(measurements_chunk), mf, 'text/csv')\n",
        "                    }\n",
        "                    upload_url = f\"{base_url}/api/v1/uploadfile_csv/campaign/{campaign_id}/station/{station_id}/sensor\"\n",
        "                    response = make_authenticated_request(\n",
        "                        method=\"POST\",\n",
        "                        url=upload_url,\n",
        "                        token=token,\n",
        "                        files=files\n",
        "                    )\n",
        "                \n",
        "                upload_time = time.time() - start_time\n",
        "                result = response.json()\n",
        "                \n",
        "                # Aggregate results\n",
        "                aggregated_results['total_sensors_processed'] += result.get('Total sensors processed', 0)\n",
        "                aggregated_results['total_measurements_added'] += result.get('Total measurements added to database', 0)\n",
        "                aggregated_results['total_processing_time'] += upload_time\n",
        "                aggregated_results['chunk_results'].append({\n",
        "                    'chunk': chunk_num,\n",
        "                    'sensors_processed': result.get('Total sensors processed', 0),\n",
        "                    'measurements_added': result.get('Total measurements added to database', 0),\n",
        "                    'upload_time': upload_time\n",
        "                })\n",
        "                \n",
        "                successful_uploads += 1\n",
        "                print(f\"  ✅ Chunk {chunk_num} uploaded successfully\")\n",
        "                print(f\"     • Sensors: {result.get('Total sensors processed', 0)}\")\n",
        "                print(f\"     • Measurements: {result.get('Total measurements added to database', 0)}\")\n",
        "                print(f\"     • Time: {upload_time:.2f}s\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                failed_uploads += 1\n",
        "                print(f\"  ❌ Chunk {chunk_num} failed: {e}\")\n",
        "                aggregated_results['chunk_results'].append({\n",
        "                    'chunk': chunk_num,\n",
        "                    'error': str(e)\n",
        "                })\n",
        "        \n",
        "        # Final results\n",
        "        print(f\"\\n📊 Chunked Upload Summary:\")\n",
        "        print(f\"  • Total chunks: {total_chunks}\")\n",
        "        print(f\"  • Successful: {successful_uploads}\")\n",
        "        print(f\"  • Failed: {failed_uploads}\")\n",
        "        print(f\"  • Total sensors processed: {aggregated_results['total_sensors_processed']:,}\")\n",
        "        print(f\"  • Total measurements added: {aggregated_results['total_measurements_added']:,}\")\n",
        "        print(f\"  • Total processing time: {aggregated_results['total_processing_time']:.2f}s\")\n",
        "        \n",
        "        if failed_uploads > 0:\n",
        "            print(f\"⚠️  {failed_uploads} chunks failed to upload\")\n",
        "        return aggregated_results\n",
        "    finally:\n",
        "        # Cleanup chunks if requested\n",
        "        if cleanup_chunks and os.path.exists(chunk_dir):\n",
        "            print(f\"🧹 Cleaning up chunk files from {chunk_dir}\")\n",
        "            shutil.rmtree(chunk_dir)\n",
        "\n",
        "def list_data_files(data_dir: str = \"./data/\") -> Dict[str, list]:\n",
        "    \"\"\"List all CSV files in the data directory.\n",
        "    Args:\n",
        "        data_dir: Directory to search for CSV files\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with lists of found files\n",
        "    \"\"\"\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"❌ Data directory not found: {data_dir}\")\n",
        "        return {\"csv_files\": [], \"sensors_files\": [], \"measurements_files\": []}\n",
        "    \n",
        "    # Find all CSV files\n",
        "    csv_pattern = os.path.join(data_dir, \"*.csv\")\n",
        "    csv_files = glob.glob(csv_pattern)\n",
        "    \n",
        "    # Categorize files\n",
        "    sensors_files = [f for f in csv_files if 'sensor' in os.path.basename(f).lower()]\n",
        "    measurements_files = [f for f in csv_files if 'measurement' in os.path.basename(f).lower()]\n",
        "    \n",
        "    print(f\"📁 Files found in {data_dir}:\")\n",
        "    print(f\"  • Total CSV files: {len(csv_files)}\")\n",
        "    print(f\"  • Sensor files: {len(sensors_files)}\")\n",
        "    print(f\"  • Measurement files: {len(measurements_files)}\")\n",
        "    if csv_files:\n",
        "        print(f\"📄 All CSV files:\")\n",
        "        for file in csv_files:\n",
        "            size = os.path.getsize(file)\n",
        "            print(f\"    - {os.path.basename(file)} ({size:,} bytes)\")\n",
        "    return {\n",
        "        \"csv_files\": csv_files,\n",
        "        \"sensors_files\": sensors_files,\n",
        "        \"measurements_files\": measurements_files\n",
        "    }\n",
        "\n",
        "def upload_data_with_auto_detection(\n",
        "    campaign_id: int,\n",
        "    station_id: int,\n",
        "    token: str,\n",
        "    data_dir: str = \"./data/\",\n",
        "    use_chunking: bool = False,\n",
        "    chunk_size: int = 10000,\n",
        "    max_file_size_mb: int = 50,\n",
        "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Upload CSV data with automatic file detection.\n",
        "    \n",
        "    Args:\n",
        "        campaign_id: ID of the target campaign\n",
        "        station_id: ID of the target station\n",
        "        token: Access token\n",
        "        data_dir: Directory containing CSV files\n",
        "        use_chunking: Whether to use chunked upload\n",
        "        chunk_size: Number of rows per chunk (if chunking)\n",
        "        max_file_size_mb: Maximum file size per chunk in MB (if chunking)\n",
        "        base_url: Base URL for the API\n",
        "        \n",
        "    Returns:\n",
        "        Upload response data\n",
        "    \"\"\"\n",
        "    print(\"=== Auto-detecting Data Files ===\")\n",
        "    files_info = list_data_files(data_dir)\n",
        "    \n",
        "    # Try to find sensors and measurements files\n",
        "    sensors_file = None\n",
        "    measurements_file = None\n",
        "    \n",
        "    # Look for standard filenames first\n",
        "    standard_sensors = os.path.join(data_dir, \"sensors.csv\")\n",
        "    standard_measurements = os.path.join(data_dir, \"measurements.csv\")\n",
        "    \n",
        "    if os.path.exists(standard_sensors):\n",
        "        sensors_file = \"sensors.csv\"\n",
        "    elif files_info[\"sensors_files\"]:\n",
        "        sensors_file = os.path.basename(files_info[\"sensors_files\"][0])\n",
        "        print(f\"🔍 Using detected sensors file: {sensors_file}\")\n",
        "    \n",
        "    if os.path.exists(standard_measurements):\n",
        "        measurements_file = \"measurements.csv\"\n",
        "    elif files_info[\"measurements_files\"]:\n",
        "        measurements_file = os.path.basename(files_info[\"measurements_files\"][0])\n",
        "        print(f\"🔍 Using detected measurements file: {measurements_file}\")\n",
        "    \n",
        "    if not sensors_file or not measurements_file:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not find required files. \"\n",
        "            f\"Sensors: {sensors_file}, Measurements: {measurements_file}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afe2d17b-abd9-4193-be54-2d43236e7f9a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nprint(\"=== Available Campaigns ===\")\\ncampaigns = get_campaigns(token)\\nprint(json.dumps(campaigns, indent=2))\\n\\nprint(\"=== Available Stations ===\")\\nstations = get_stations(CAMPAIGN_ID, token)\\nprint(json.dumps(stations, indent=2))\\n'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_campaigns(token: str, base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\") -> Dict[str, Any]:\n",
        "    \"\"\"Get list of available campaigns.\"\"\"\n",
        "    url = f\"{base_url}/api/v1/campaigns\"\n",
        "    response = make_authenticated_request(\"GET\", url, token)\n",
        "    return response.json()\n",
        "\n",
        "def get_stations(campaign_id: int, token: str, base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\") -> Dict[str, Any]:\n",
        "    \"\"\"Get list of stations for a campaign.\"\"\"\n",
        "    url = f\"{base_url}/api/v1/campaigns/{campaign_id}/stations\"\n",
        "    response = make_authenticated_request(\"GET\", url, token)\n",
        "    return response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e77a7f7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📁 Files found in ./data/:\n",
            "  • Total CSV files: 2\n",
            "  • Sensor files: 1\n",
            "  • Measurement files: 1\n",
            "📄 All CSV files:\n",
            "    - measurements.csv (906,949 bytes)\n",
            "    - sensors.csv (173 bytes)\n",
            "📈 Upload Progress Estimation:\n",
            "  • Total data: 0.87 MB, 8,964 rows\n",
            "  • Estimated upload time: 1.7 seconds\n",
            "=== Chunked CSV Data Upload ===\n",
            "Campaign ID: 12\n",
            "Station ID: 39\n",
            "Chunk size: 6,000 rows\n",
            "Max chunk file size: 30 MB\n",
            "\n",
            "📁 File Analysis:\n",
            "  • Sensors: 3 rows, 0.00 MB\n",
            "  • Measurements: 8,961 rows, 0.86 MB\n",
            "\n",
            "--- Chunking Sensors File ---\n",
            "📦 Chunking sensors.csv:\n",
            "  • Total rows: 3\n",
            "  • File size: 0.00 MB\n",
            "  • Chunk size: 6,000 rows\n",
            "  • Estimated chunks: 1\n",
            "  ✓ Created chunk 1: 3 rows, 0.00 MB\n",
            "📦 Created 1 chunks in /var/folders/ps/dx2yrk_1117grf32kqlw9qyh0000gq/T/upload_chunks_6eegcnam/sensors\n",
            "\n",
            "--- Chunking Measurements File ---\n",
            "📦 Chunking measurements.csv:\n",
            "  • Total rows: 8,961\n",
            "  • File size: 0.86 MB\n",
            "  • Chunk size: 6,000 rows\n",
            "  • Estimated chunks: 2\n",
            "  ✓ Created chunk 1: 6000 rows, 0.58 MB\n",
            "  ✓ Created chunk 2: 2961 rows, 0.29 MB\n",
            "📦 Created 2 chunks in /var/folders/ps/dx2yrk_1117grf32kqlw9qyh0000gq/T/upload_chunks_6eegcnam/measurements\n",
            "\n",
            "📤 Uploading 2 chunk pairs...\n",
            "\n",
            "--- Uploading Chunk 1/2 ---\n",
            "  ✅ Chunk 1 uploaded successfully\n",
            "     • Sensors: 3\n",
            "     • Measurements: 0\n",
            "     • Time: 5.66s\n",
            "\n",
            "--- Uploading Chunk 2/2 ---\n",
            "  ✅ Chunk 2 uploaded successfully\n",
            "     • Sensors: 3\n",
            "     • Measurements: 0\n",
            "     • Time: 2.80s\n",
            "\n",
            "📊 Chunked Upload Summary:\n",
            "  • Total chunks: 2\n",
            "  • Successful: 2\n",
            "  • Failed: 0\n",
            "  • Total sensors processed: 6\n",
            "  • Total measurements added: 0\n",
            "  • Total processing time: 8.46s\n",
            "🧹 Cleaning up chunk files from /var/folders/ps/dx2yrk_1117grf32kqlw9qyh0000gq/T/upload_chunks_6eegcnam\n",
            "\n",
            "📊 Final Upload Statistics:\n",
            "  • Actual upload time: 8.59 seconds\n",
            "  • Average speed: 0.10 MB/s\n",
            "  • Rows per second: 1044\n"
          ]
        }
      ],
      "source": [
        "# List available files\n",
        "files_info = list_data_files(\"./data/\")\n",
        "# Analyze file sizes\n",
        "sensors_path = \"./data/sensors.csv\"\n",
        "measurements_path = \"./data/measurements.csv\"\n",
        "if os.path.exists(sensors_path) and os.path.exists(measurements_path):\n",
        "    sensors_info = get_file_info(sensors_path)\n",
        "    measurements_info = get_file_info(measurements_path)\n",
        "    total_size_mb = sensors_info['size_mb'] + measurements_info['size_mb']\n",
        "    total_rows = sensors_info['row_count'] + measurements_info['row_count']\n",
        "    # Start upload with progress tracking\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        result = upload_csv_data_chunked(\n",
        "            campaign_id=campaign_id,\n",
        "            station_id=station_id,\n",
        "            token=token,\n",
        "            data_dir=\"./data/\",\n",
        "            sensors_filename=\"sensors.csv\",\n",
        "            measurements_filename=\"measurements.csv\",\n",
        "            chunk_size=6000,\n",
        "            max_file_size_mb=30\n",
        "        )\n",
        "        total_time = time.time() - start_time\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Progress monitored upload failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d29b662",
      "metadata": {},
      "source": [
        "## Create Measurement\n",
        "The create_measurement function allows you to post a single measurement to the Upstream API for a specific sensor within a campaign and station."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c92fc98c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_measurement(\n",
        "    campaign_id: int,\n",
        "    station_id: int, \n",
        "    sensor_id: int,\n",
        "    measurement_data: Dict[str, Any],\n",
        "    token: str,\n",
        "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Create a single measurement for a sensor.\"\"\"\n",
        "    url = f\"{base_url}/api/v1/campaigns/{campaign_id}/stations/{station_id}/sensors/{sensor_id}/measurements\"\n",
        "    response = make_authenticated_request(\"POST\", url, token, json=measurement_data)\n",
        "    return response.json()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "376d75bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "campaign_id = 12\n",
        "station_id = 39  \n",
        "sensor_id = 9664\n",
        "\n",
        "# Measurement data\n",
        "measurement_data = {\n",
        "    \"variablename\": \"Rain Increement\",\n",
        "    \"collectiontime\": \"2024-01-15T10:37:00\",\n",
        "    \"variabletype\": \"float\", \n",
        "    \"description\": \"Rain Increment measurement\",\n",
        "    \"measurementvalue\": 25.3,\n",
        "    \"geometry\":  'POINT(10.12345 20.54321)'\n",
        "    \n",
        "}\n",
        "result = create_measurement(\n",
        "            campaign_id=campaign_id,\n",
        "            station_id=station_id,\n",
        "            sensor_id=sensor_id,\n",
        "            measurement_data=measurement_data,\n",
        "            token=token\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e2fa823",
      "metadata": {},
      "source": [
        "## 7. Best Practices\n",
        "\n",
        "1. **File Preparation:**\n",
        "   - Validate your CSV files before upload\n",
        "   - Ensure sensor aliases match between files\n",
        "   - Use consistent timestamp formats\n",
        "\n",
        "2. **Error Handling:**\n",
        "   - Always wrap API calls in try-catch blocks\n",
        "   - Check file existence before upload\n",
        "   - Validate response status codes\n",
        "\n",
        "3. **Security:**\n",
        "   - Never hardcode credentials in notebooks\n",
        "   - Store tokens securely\n",
        "   - Use environment variables for sensitive data\n",
        "\n",
        "4. **Performance:**\n",
        "   - Keep files under 500 MB for optimal performance\n",
        "   - Use batch uploads for large datasets\n",
        "   - Monitor upload progress and statistics"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
